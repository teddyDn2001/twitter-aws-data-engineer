# AWS Twitter Data Engineering Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](./LICENSE)
![Status](https://img.shields.io/badge/status-active-brightgreen)
![Made with](https://img.shields.io/badge/made%20with-Python%20%26%20Spark-blue)

> Endâ€‘toâ€‘end data engineering pipeline trÃªn AWS Ä‘á»ƒ thu tháº­p, xá»­ lÃ½, catalog, phÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u Twitter.  
> Triá»ƒn khai theo mÃ´ hÃ¬nh Data Lake (Raw â†’ Cleansed â†’ Processed), cÃ³ thá»ƒ má»Ÿ rá»™ng cho doanh nghiá»‡p.

---

## ğŸ“š Má»¥c lá»¥c
- [1. Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [2. Kiáº¿n trÃºc](#-kiáº¿n-trÃºc)
- [3. Luá»“ng xá»­ lÃ½ dá»¯ liá»‡u](#-luá»“ng-xá»­-lÃ½-dá»¯-liá»‡u)
- [4. CÃ¡ch cháº¡y (Demo)](#-cÃ¡ch-cháº¡y-demo)
- [5. SQL máº«u (Athena)](#-sql-máº«u-athena)
- [6. CÃ´ng nghá»‡ sá»­ dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [7. HÆ°á»›ng phÃ¡t triá»ƒn](#-hÆ°á»›ng-phÃ¡t-triá»ƒn)
- [8. Cáº¥u trÃºc repo](#-cáº¥u-trÃºc-repo)
- [9. TÃ¡c giáº£](#-tÃ¡c-giáº£)
- [10. License](#-license)

---

## ğŸ” Giá»›i thiá»‡u
Dá»± Ã¡n mÃ´ phá»ng quy trÃ¬nh **ETL/ELT hiá»‡n Ä‘áº¡i**:
- Thu tháº­p dá»¯ liá»‡u Twitter (script Python hoáº·c API).
- LÆ°u trá»¯ trÃªn **S3** (Raw â†’ Cleansed â†’ Processed).
- Xá»­ lÃ½/lÃ m sáº¡ch báº±ng **Spark trÃªn EMR** (trigger qua **Airflow**).
- Táº¡o **Glue Data Catalog** & query báº±ng **Athena**.
- Táº£i vÃ o **Redshift** (tuá»³ chá»n) cho analytics chuyÃªn sÃ¢u.
- Trá»±c quan hÃ³a báº±ng **QuickSight** hoáº·c **PowerBI**.

> **Má»¥c tiÃªu:** Portfolio Data Engineer/Analyst cÃ³ tÃ­nh thá»±c chiáº¿n, dá»… má»Ÿ rá»™ng.

---

## ğŸ— Kiáº¿n trÃºc
Twitter (API/Crawler)
|
v
S3 / raw  â€“â€“>  EMR (Spark jobs)  â€“â€“>  S3 / cleansed  â€“â€“>  S3 / processed
|                                   |
v                                   v
Glue Crawler  â€”â€”â€”â€”â€”â€”>  Glue Catalog
|                                   |
+â€“â€“â€“â€“â€“> Athena  <â€“â€“â€“â€“â€“â€“â€“+
|
+â€“> Redshift (tuá»³ chá»n)
|
+â€“> QuickSight / PowerBI
Orchestration: Airflow (DAG: twitter_full_etl_pipeline)


---

## ğŸ”„ Luá»“ng xá»­ lÃ½ dá»¯ liá»‡u

1. **Crawl dá»¯ liá»‡u Twitter**  
   - Script: `crawl_tweets.py`  
   - Output: `s3://<bucket>/raw/...`

2. **Spark ETL trÃªn EMR (Raw â†’ Cleansed â†’ Processed)**  
   - Job: `spark_elt_twitter.py` (chuáº©n hoÃ¡ schema, lÃ m sáº¡ch, chuáº©n Parquet)
   - Trigger bá»Ÿi **Airflow DAG** `twitter_full_etl_pipeline.py`

3. **Data Catalog vá»›i Glue Crawler**  
   - Crawler chá»‰ vÃ o prefixes `s3://.../cleansed/` vÃ  `s3://.../processed/`
   - Táº¡o báº£ng trong **Glue Database** (vÃ­ dá»¥: `social_media` / `twitter_analytics`)

4. **Query báº±ng Athena / Load vÃ o Redshift**  
   - SQL truy váº¥n trá»±c tiáº¿p data processed  
   - Copy vÃ o Redshift (náº¿u cáº§n BI náº·ng, phÃ¢n tÃ­ch OLAP)

5. **BI Layer**  
   - **QuickSight** (AWS-native) hoáº·c **PowerBI** (káº¿t ná»‘i Athena ODBC)

---

## ğŸ§ª CÃ¡ch cháº¡y (Demo)

### YÃªu cáº§u
- Python 3.9+
- ÄÃ£ cáº¥u hÃ¬nh **AWS CLI**: `aws configure`
- Quyá»n IAM cho S3/EMR/Glue/Athena/Redshift (náº¿u dÃ¹ng)

### 1) Clone project
```bash
git clone https://github.com/<your-username>/twitter-aws-data-engineer.git
cd twitter-aws-data-engineer

2) CÃ i Ä‘áº·t Python packages
pip install -r requirements.txt

3) Cháº¡y Airflow cá»¥c bá»™ (tuá»³ chá»n demo)
export AIRFLOW_HOME=~/airflow
airflow db init
# Copy DAG vÃ o thÆ° má»¥c:
#   cp airflow_dags.py  ~/airflow/dags/twitter_full_etl_pipeline.py
airflow webserver -p 8080 &
airflow scheduler &
# Má»Ÿ http://localhost:8080 rá»“i báº­t DAG

4) Spark job trÃªn EMR (production)
	â€¢	Upload scripts vÃ o s3://<bucket>/scripts/
	â€¢	Airflow â†’ EmrAddStepsOperator gá»i spark-submit vá»›i input/output tÆ°Æ¡ng á»©ng:
	â€¢	Input: s3://<bucket>/raw/
	â€¢	Output: s3://<bucket>/cleansed/ â†’ s3://<bucket>/processed/

5) Glue & Athena
	â€¢	Táº¡o Glue Database (vÃ­ dá»¥ social_media)
	â€¢	Táº¡o Glue Crawler â†’ trá» vÃ o cleansed/ & processed/ â†’ cháº¡y crawler
	â€¢	VÃ o Athena â†’ chá»n database vá»«a táº¡o â†’ cháº¡y SQL (vÃ­ dá»¥ á»Ÿ bÃªn dÆ°á»›i)

â¸»

ğŸ§¾ SQL máº«u (Athena)
-- Top 10 user tweet nhiá»u nháº¥t
SELECT user_name, COUNT(*) AS tweet_count
FROM twitter_processed
GROUP BY user_name
ORDER BY tweet_count DESC
LIMIT 10;

-- Hashtag phá»• biáº¿n
SELECT lower(hashtag) AS tag, COUNT(*) as cnt
FROM twitter_processed
CROSS JOIN UNNEST(hashtags) AS t(hashtag)
GROUP BY lower(hashtag)
ORDER BY cnt DESC
LIMIT 20;

-- TÆ°Æ¡ng tÃ¡c theo ngÃ y
SELECT date_trunc('day', created_at) AS d, COUNT(*) AS tweets
FROM twitter_processed
GROUP BY 1
ORDER BY 1;

ğŸ›  CÃ´ng nghá»‡ sá»­ dá»¥ng
	â€¢	AWS: S3, EMR (Spark), Glue Crawler & Catalog, Athena, (tuá»³ chá»n) Redshift, QuickSight
	â€¢	Processing: Apache Spark (PySpark)
	â€¢	Orchestration: Apache Airflow
	â€¢	NgÃ´n ngá»¯: Python
	â€¢	BI: PowerBI / QuickSight

â¸»

ğŸš€ HÆ°á»›ng phÃ¡t triá»ƒn
	â€¢	ThÃªm ML/AI: Sentiment Analysis, Topic Modeling (SageMaker / Bedrock)
	â€¢	Streaming realâ€‘time vá»›i Kafka/MSK
	â€¢	Redshift Spectrum / Iceberg table Ä‘á»ƒ tá»‘i Æ°u chi phÃ­ query

â¸»

ğŸ—‚ Cáº¥u trÃºc repo
twitter-aws-data-engineer/
â”œâ”€â”€ airflow_dags.py                 # DAG Airflow (gá»i EMR + Redshift COPY)
â”œâ”€â”€ spark_elt_twitter.py            # Spark job: raw -> cleansed -> processed
â”œâ”€â”€ crawl_tweets.py                 # (Tuá»³ chá»n) Crawl dá»¯ liá»‡u Twitter
â”œâ”€â”€ bootstrap.sh                    # (Tuá»³ chá»n) Script bootstrap EMR
â”œâ”€â”€ PROJECT DATA ENGINEERING.docx   # Proposal (báº£n tháº£o)
â”œâ”€â”€ README.md
â””â”€â”€ images/
    â”œâ”€â”€ architecture.png            # (Tuá»³ chá»n) Kiáº¿n trÃºc
    â””â”€â”€ dashboard-demo.png          # (Tuá»³ chá»n) Dashboard

ğŸ‘¤ TÃ¡c giáº£
	â€¢	TÃªn: Doanh Teddy
	â€¢	LinkedIn: https://www.linkedin.com/in/adoan2201/
	â€¢	GitHub: https://github.com/teddyDn2001

â¸»

ğŸ“œ License

PhÃ¡t hÃ nh dÆ°á»›i giáº¥y phÃ©p MIT â€“ xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.